########################################################################
# Pseudo-labeling NAB_KAGGLE using meta-features + RandomForest
#
# - final_labeled_dataset_converted.csv ê¸°ë°˜ìœ¼ë¡œ ë©”íƒ€ ëª¨ë¸ í•™ìŠµ
# - NAB_KAGGLEì€ anomaly ì—†ëŠ” íŒŒì¼ë„ ê°•ì œë¡œ ë¡œë“œ (ë°©ë²• B)
# - ì˜ˆì¸¡ëœ ë¼ë²¨ì„ ê²°í•© â†’ final_labeled_dataset_converted_v2.csv ìƒì„±
########################################################################

import os
import numpy as np
import pandas as pd

from utils.data_loader import DataLoader
from utils.config import TSB_data_path, TSB_metrics_path

from scipy.stats import skew, kurtosis
from sklearn.ensemble import RandomForestClassifier


########################################################################
# 1) Feature extractor
########################################################################
def extract_features(ts: np.ndarray) -> np.ndarray:
    """NaN ë°©ì§€ ê¸°ëŠ¥ì´ í¬í•¨ëœ robust meta-feature extractor"""
    ts = np.asarray(ts, dtype=float).flatten()

    # ê¸°ë³¸ ê°’ ê³„ì‚°
    length = len(ts)
    mean = np.mean(ts)
    std = np.std(ts)
    vmin = np.min(ts)
    vmax = np.max(ts)

    # skew, kurtosisëŠ” identical value TSì—ì„œ NaN ë°œìƒ ê°€ëŠ¥
    try:
        s = skew(ts, nan_policy="omit")
    except:
        s = 0.0

    try:
        k = kurtosis(ts, nan_policy="omit")
    except:
        k = 0.0

    # percentile ê³„ì‚°ë„ NaN ë°œìƒ ê°€ëŠ¥
    try:
        q25 = np.percentile(ts, 25)
        q50 = np.percentile(ts, 50)
        q75 = np.percentile(ts, 75)
    except:
        q25 = q50 = q75 = 0.0

    # energy
    try:
        energy = np.mean(ts ** 2)
    except:
        energy = 0.0

    feats = np.array([
        length, mean, std, vmin, vmax,
        s, k, q25, q50, q75, energy
    ], dtype=float)

    # ğŸ”¥ ë§ˆì§€ë§‰ ë°©ì–´: NaN ë˜ëŠ” infê°€ ìˆìœ¼ë©´ ëª¨ë‘ 0ìœ¼ë¡œ ì¹˜í™˜
    feats = np.nan_to_num(feats, nan=0.0, posinf=0.0, neginf=0.0)

    return feats


########################################################################
# 2) NAB_KAGGLE ì „ìš© raw loader (anomaly ì—¬ë¶€ì™€ ìƒê´€ì—†ì´ ëª¨ë“  íŒŒì¼ ë¡œë“œ)
########################################################################
def load_timeseries_raw(dir_path):
    ts_list = []
    fname_list = []

    for root, dirs, files in os.walk(dir_path):
        for f in files:
            if f.endswith(".out"):
                full = os.path.join(root, f)
                try:
                    curr = np.loadtxt(full, delimiter=",")
                except:
                    continue

                # ë°˜ë“œì‹œ 2ì»¬ëŸ¼ í˜•íƒœì—¬ì•¼ í•¨
                if curr.ndim != 2 or curr.shape[1] != 2:
                    continue

                # ì²« ë²ˆì§¸ ì»¬ëŸ¼ë§Œ ì‚¬ìš© (TS ê°’)
                ts_list.append(curr[:, 0])

                # dataset/file í˜•íƒœì˜ relative path ë°˜í™˜
                rel = full.replace(TSB_data_path, "").lstrip("/")
                fname_list.append(rel)

    return ts_list, fname_list


########################################################################
# 3) Main
########################################################################
def main():
    # --------------------------------------------------------------
    # (0) ê¸°ì¡´ ë¼ë²¨ ë¡œë“œ
    # --------------------------------------------------------------
    label_path = os.path.join(TSB_metrics_path, "final_labeled_dataset_converted.csv")
    print(f"[INFO] Loading labels from: {label_path}")
    df = pd.read_csv(label_path)

    # key ìƒì„±
    df["key"] = df["dataset"].astype(str) + "/" + df["filename"].astype(str)

    # ì¤‘ë³µ ì œê±°
    before = len(df)
    df = df.drop_duplicates(subset=["key"], keep="first")
    removed = before - len(df)
    print(f"[INFO] Removed duplicated rows: {removed}")

    df = df.set_index("key")

    # --------------------------------------------------------------
    # (1) TSB ì „ì²´ ë°ì´í„° ì¤‘ NAB_KAGGLE ì œì™¸í•˜ê³  ë¡œë”©
    # --------------------------------------------------------------
    dataloader = DataLoader(TSB_data_path)
    datasets_all = dataloader.get_dataset_names()

    datasets_train = [d for d in datasets_all if d != "NAB_KAGGLE"]
    datasets_nab = ["NAB_KAGGLE"]

    print("[INFO] Train datasets:", datasets_train)
    print("[INFO] NAB_KAGGLE dataset:", datasets_nab)

    # Train dataset ë¡œë“œ
    x_train_list, y_dummy, fnames_train = dataloader.load(datasets_train)

    # --------------------------------------------------------------
    # (2) NAB_KAGGLE ì „ìš© raw ë¡œë”© (ë°©ë²• B í•µì‹¬)
    # --------------------------------------------------------------
    nab_path = os.path.join(TSB_data_path, "NAB_KAGGLE")
    x_nab_list, fnames_nab = load_timeseries_raw(nab_path)

    # --------------------------------------------------------------
    # (3) Feature + Label ì •ë¦¬
    # --------------------------------------------------------------
    X_train = []
    y_train = []

    # Train ë°ì´í„° â†’ feature + label
    for ts, fname in zip(x_train_list, fnames_train):
        dataset_name = fname.split("/")[0]
        filename = fname.split("/", 1)[1]
        key = f"{dataset_name}/{filename}"

        if key not in df.index:
            continue

        label = df.loc[key, "label"]
        feats = extract_features(ts)

        X_train.append(feats)
        y_train.append(label)

    X_train = np.asarray(X_train, dtype=float)
    y_train = np.asarray(y_train, dtype=object)

    print(f"[INFO] Training samples: {X_train.shape[0]}")

    # NAB_KAGGLE feature
    X_nab = []
    nab_keys = []

    for ts, fname in zip(x_nab_list, fnames_nab):
        feats = extract_features(ts)
        X_nab.append(feats)
        nab_keys.append(fname)

    X_nab = np.asarray(X_nab, dtype=float)
    nab_keys = np.asarray(nab_keys, dtype=object)

    print(f"[INFO] NAB_KAGGLE samples to label: {len(X_nab)}")

    if len(X_nab) == 0:
        print("[WARN] No NAB_KAGGLE files found for labeling.")
        return

    # --------------------------------------------------------------
    # (4) RandomForest ë©”íƒ€ ëª¨ë¸ í•™ìŠµ
    # --------------------------------------------------------------
    print("[INFO] Training RandomForest model...")
    clf = RandomForestClassifier(
        n_estimators=300,
        n_jobs=-1,
        class_weight="balanced_subsample",
        random_state=42
    )
    clf.fit(X_train, y_train)

    # --------------------------------------------------------------
    # (5) NAB_KAGGLE ì˜ˆì¸¡
    # --------------------------------------------------------------
    print("[INFO] Predicting labels for NAB_KAGGLE...")
    nab_pred = clf.predict(X_nab)

    # ê²°ê³¼ dataframe
    nab_rows = []
    for key, lab in zip(nab_keys, nab_pred):
        dataset_name = key.split("/")[0]
        filename = key.split("/", 1)[1]
        nab_rows.append({
            "dataset": dataset_name,
            "filename": filename,
            "label": lab
        })

    nab_df = pd.DataFrame(nab_rows)

    print("[INFO] NAB_KAGGLE predicted labels (head):")
    print(nab_df.head())

    # --------------------------------------------------------------
    # (6) ê¸°ì¡´ ë¼ë²¨ + NAB_KAGGLE ë¼ë²¨ merge
    # --------------------------------------------------------------
    original = pd.read_csv(label_path)
    merged = pd.concat([original, nab_df], ignore_index=True)

    merged["key"] = merged["dataset"].astype(str) + "/" + merged["filename"].astype(str)
    merged = merged.drop_duplicates(subset=["key"], keep="first").drop(columns=["key"])

    out_path = os.path.join(TSB_metrics_path, "final_labeled_dataset_converted_v2.csv")
    merged.to_csv(out_path, index=False)

    print(f"[INFO] Saved merged label file â†’ {out_path}")


########################################################################
if __name__ == "__main__":
    main()